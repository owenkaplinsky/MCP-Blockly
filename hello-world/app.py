from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
import gradio as gr
import uvicorn
import asyncio

app = FastAPI()

# Allow Blockly (running on localhost:8080) to connect
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:8080", "http://127.0.0.1:8080"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Authoritative chat history (the one true state)
history = []

# Live Python code string generated from Blockly
latest_blockly_code = ""

# A queue for assistant replies (so Gradio waits until Blockly has responded)
assistant_queue = asyncio.Queue()


@app.post("/update_code")
async def update_code(request: Request):
    """Receives live Python code generated by Blockly."""
    global latest_blockly_code
    data = await request.json()
    latest_blockly_code = data.get("code", "")
    print("\n[FASTAPI] Updated Blockly code:\n", latest_blockly_code)
    return {"ok": True}


def execute_blockly_logic(user_message: str) -> str:
    """
    Executes the latest Blockly-generated Python code in a controlled environment.
    Expects the Blockly code to define a function called on_user_send(user_message)
    that returns or prints the assistant reply.
    """
    global latest_blockly_code

    if not latest_blockly_code.strip():
        print("[EXECUTION] No Blockly code yet.")
        return "(no logic yet)"

    try:
        local_vars = {}
        # Execute the code safely (creates function definitions, etc.)
        exec(latest_blockly_code, {}, local_vars)
        # Call the defined handler function, if it exists
        if "on_user_send" in local_vars:
            result = local_vars["on_user_send"](user_message)
            if result is None:
                result = "(no reply)"
            return str(result)
        else:
            print("[EXECUTION] No on_user_send() found in Blockly code.")
            return "(missing on_user_send)"
    except Exception as e:
        print("[EXECUTION ERROR]", e)
        return f"(error: {e})"


def build_interface():
    with gr.Blocks() as demo:
        chatbot = gr.Chatbot(type="messages", label="Assistant")
        msg = gr.Textbox(placeholder="Type a message and press Enter")

        async def process_message(message):
            global history

            # 1. Add user message
            history.append({"role": "user", "content": message})
            print(f"[USER] {message!r}")
            yield "", history

            # 2. Compute assistant response using current Blockly logic
            assistant_text = execute_blockly_logic(message)
            print(f"[ASSISTANT] {assistant_text!r}")

            # 3. Add assistant message
            history.append({"role": "assistant", "content": assistant_text})
            yield "", history

        msg.submit(process_message, [msg], [msg, chatbot], queue=True)

        clear_btn = gr.Button("Reset chat")
        clear_btn.click(lambda: ([], ""), None, [chatbot, msg], queue=False)

    return demo


demo = build_interface()
app = gr.mount_gradio_app(app, demo, path="/")

if __name__ == "__main__":
    print("[BOOT] Running Gradio+FastAPI combo on http://127.0.0.1:7860")
    uvicorn.run(app, host="0.0.0.0", port=7860)
